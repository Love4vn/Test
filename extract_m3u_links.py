#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
M3U Stream Link Extractor for GitHub Actions
T·ª± ƒë·ªông l·∫•y link m3u/m3u8 t·ª´ trang web
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import json
from datetime import datetime
import os

class M3UStreamExtractor:
    def __init__(self, urls, output_dir='output'):
        self.urls = urls if isinstance(urls, list) else [urls]
        self.output_dir = output_dir
        self.all_links = []
        self.results = {}
        
        # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a t·ªìn t·∫°i
        os.makedirs(output_dir, exist_ok=True)
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        }

    def fetch_page(self, url):
        """T·∫£i trang web"""
        try:
            print(f"üì• ƒêang t·∫£i: {url}")
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            print(f"‚úÖ T·∫£i th√†nh c√¥ng!")
            return response.text
        except Exception as e:
            print(f"‚ùå L·ªói: {e}")
            return None

    def extract_m3u_links(self, html_content, base_url):
        """Tr√≠ch xu·∫•t link m3u t·ª´ HTML"""
        links = set()
        
        # Ph∆∞∆°ng ph√°p 1: Regex t√¨m link m3u8/m3u tr·ª±c ti·∫øp
        pattern = r'https?://[^"]*?\.m3u8?(?:\?[^"]*)?'
        found = re.findall(pattern, html_content)
        links.update(found)
        
        # Ph∆∞∆°ng ph√°p 2: T√¨m trong th·∫ª <a>
        soup = BeautifulSoup(html_content, 'html.parser')
        for tag in soup.find_all('a', href=True):
            href = tag['href']
            if 'm3u' in href.lower():
                full_url = urljoin(base_url, href)
                links.add(full_url)
        
        # Ph∆∞∆°ng ph√°p 3: T√¨m trong iframe
        for iframe in soup.find_all('iframe', src=True):
            src = iframe['src']
            if src and ('m3u' in src.lower() or 'stream' in src.lower()):
                full_url = urljoin(base_url, src)
                links.add(full_url)
        
        # Ph∆∞∆°ng ph√°p 4: T√¨m trong c√°c script tags
        for script in soup.find_all('script'):
            if script.string:
                found = re.findall(pattern, script.string)
                links.update(found)
        
        return list(links)

    def extract_channel_info(self, html_content, base_url):
        """Tr√≠ch xu·∫•t th√¥ng tin k√™nh ph√°t s√≥ng"""
        soup = BeautifulSoup(html_content, 'html.parser')
        channels = []
        
        # T√¨m c√°c link c√≥ li√™n quan ƒë·∫øn stream/play
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text(strip=True)
            
            if any(kw in text.lower() or kw in href.lower() 
                   for kw in ['play', 'stream', 'watch', 'live', 'sports', 'tv']):
                channels.append({
                    'name': text[:100],
                    'url': urljoin(base_url, href),
                })
        
        return channels[:50]  # Gi·ªõi h·∫°n 50 k√™nh

    def process_url(self, url):
        """X·ª≠ l√Ω m·ªôt URL"""
        html = self.fetch_page(url)
        if not html:
            return None
        
        m3u_links = self.extract_m3u_links(html, url)
        channels = self.extract_channel_info(html, url)
        
        self.results[url] = {
            'timestamp': datetime.now().isoformat(),
            'total_links': len(m3u_links),
            'links': m3u_links,
            'channels': channels
        }
        
        self.all_links.extend(m3u_links)
        
        print(f"  üìå T√¨m th·∫•y {len(m3u_links)} link m3u")
        return m3u_links

    def save_m3u_playlist(self):
        """L∆∞u m3u playlist"""
        filename = os.path.join(self.output_dir, 'm3u_playlist.m3u')
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('#EXTM3U\n')
            
            for idx, link in enumerate(self.all_links, 1):
                f.write(f'#EXTINF:-1,Stream {idx}\n')
                f.write(f'{link}\n')
        
        print(f"‚úÖ L∆∞u playlist: {filename}")
        return filename

    def save_txt_list(self):
        """L∆∞u danh s√°ch link d·∫°ng text"""
        filename = os.path.join(self.output_dir, 'm3u_links.txt')
        
        with open(filename, 'w', encoding='utf-8') as f:
            for link in self.all_links:
                f.write(f'{link}\n')
        
        print(f"‚úÖ L∆∞u danh s√°ch: {filename}")
        return filename

    def save_json_data(self):
        """L∆∞u d·ªØ li·ªáu JSON"""
        filename = os.path.join(self.output_dir, 'm3u_data.json')
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'total_urls': len(self.urls),
            'total_links': len(self.all_links),
            'details': self.results,
            'links': list(set(self.all_links))  # Lo·∫°i b·ªè duplicate
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ L∆∞u JSON: {filename}")
        return filename

    def save_readme(self):
        """T·∫°o README"""
        filename = os.path.join(self.output_dir, 'README.md')
        
        content = f"""# M3U Stream Links

**C·∫≠p nh·∫≠t l·∫ßn cu·ªëi:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Th·ªëng k√™
- **T·ªïng URL ki·ªÉm tra:** {len(self.urls)}
- **T·ªïng link t√¨m th·∫•y:** {len(self.all_links)}

## Danh s√°ch URL
"""
        for url in self.urls:
            links_count = self.results[url]['total_links'] if url in self.results else 0
            content += f"- {url} ({links_count} links)\n"
        
        content += f"\n## File output\n"
        content += f"- `m3u_playlist.m3u` - Playlist m3u format\n"
        content += f"- `m3u_links.txt` - Danh s√°ch link (m·ªói d√≤ng m·ªôt link)\n"
        content += f"- `m3u_data.json` - D·ªØ li·ªáu JSON ƒë·∫ßy ƒë·ªß\n"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"‚úÖ T·∫°o README: {filename}")

    def run(self):
        """Ch·∫°y extractor"""
        print("\n" + "="*60)
        print("üé¨ M3U STREAM LINK EXTRACTOR")
        print("="*60)
        
        for url in self.urls:
            self.process_url(url)
        
        # L∆∞u k·∫øt qu·∫£
        self.save_m3u_playlist()
        self.save_txt_list()
        self.save_json_data()
        self.save_readme()
        
        print("\n" + "="*60)
        print(f"‚úÖ Ho√†n th√†nh! T√¨m th·∫•y {len(self.all_links)} link")
        print("="*60 + "\n")


def main():
    # Danh s√°ch URL c·∫ßn extract
    urls = [
        'https://streamsports99.su/',
        # Th√™m URL kh√°c ·ªü ƒë√¢y
    ]
    
    # T·∫°o extractor v√† ch·∫°y
    extractor = M3UStreamExtractor(urls, output_dir='output')
    extractor.run()


if __name__ == '__main__':
    main()